{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the Shyft Timeseries API\n",
    "\n",
    "### partition_by and percentiles functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first you should import the third-party python modules which you'll use later on\n",
    "# the first line enables that figures are shown inline, directly in the notebook\n",
    "%pylab inline\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "# set the path for your shyft build\n",
    "# this should point to the directory that is created\n",
    "# when you clone shyft, assuming you have built shyft\n",
    "# there and not installed it to your system python\n",
    "shyft_path = os.path.abspath(\"../../../shyft\")\n",
    "sys.path.insert(0, shyft_path)\n",
    "\n",
    "# you could achieve the same by setting a PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shyft.api import Calendar\n",
    "from shyft.api import deltahours\n",
    "from shyft.api import TimeAxis\n",
    "from shyft.api import point_interpretation_policy as fx_policy\n",
    "from shyft.api import DoubleVector\n",
    "from shyft.api import TsVector\n",
    "from shyft.api import TimeSeries\n",
    "from shyft.api import percentiles\n",
    "from shyft.api import statistics_property as stat_p\n",
    "from shyft.orchestration import plotting as splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo partition_by and percentiles function\n",
    "\n",
    "utc = Calendar()\n",
    "t0 = utc.time(1930, 9, 1)\n",
    "dt = deltahours(1)\n",
    "n = utc.diff_units(t0, utc.time(2016, 9, 1), dt)\n",
    "\n",
    "ta = TimeAxis(t0, dt, n)\n",
    "\n",
    "# just generate some data that is alive\n",
    "x = np.linspace(start=0.0,stop=np.pi*200,num=len(ta))\n",
    "\n",
    "pattern_values = DoubleVector.from_numpy(500.0 + x*np.sin(x)+x/10+ 1000.0*np.random.normal(0,0.1,len(ta))) # increasing values\n",
    "\n",
    "src_ts = TimeSeries(ta=ta, values=pattern_values, point_fx=fx_policy.POINT_AVERAGE_VALUE)\n",
    "\n",
    "partition_t0 = utc.time(2016, 9, 1) # we want our percentiles to align to this particular time, for presentation\n",
    "n_partitions = 80\n",
    "partition_interval = Calendar.YEAR\n",
    "\n",
    "# call .partition_by that does the magic of aligning using the time_shift function\n",
    "# returns TsVector,\n",
    "# where all TsVector[i].index_of(partition_t0)\n",
    "# is equal to the index ix for which the TsVector[i].value(ix) \n",
    "# correspond to start value of that particular partition.\n",
    "#  in short: moves the years to a common start-point that you specify, utilizing the time_shift function\n",
    "#            still-keeping a reference to the underlying time-series (no copy occurs, just mirroring)\n",
    "#\n",
    "ts_partitions = src_ts.partition_by(utc, t0, partition_interval, n_partitions, partition_t0)\n",
    "\n",
    "\n",
    "# Now finally, try percentiles on the partitions that have the proper ts-vector size\n",
    "\n",
    "wanted_percentiles = [stat_p.MIN_EXTREME, 0, 10, 25, stat_p.AVERAGE, 50, 75, 90, 100,stat_p.MAX_EXTREME] \n",
    "\n",
    "t0_view = utc.add(partition_t0,Calendar.MONTH,1) # note to Eli; this is how we can present one year from 1.10\n",
    "ta_percentiles = TimeAxis(t0_view, deltahours(24), 365) # this is the time-axis that we want aggregate on, day-average\n",
    "p_result = percentiles(ts_partitions,ta_percentiles,wanted_percentiles)\n",
    "\n",
    "# finally some plot, including one particular year (note the .average trick)\n",
    "common_timestamps = [datetime.datetime.utcfromtimestamp(p.start) for p in ta_percentiles]\n",
    "fig,ax=plt.subplots(figsize=(16,8))\n",
    "splt.set_calendar_formatter(utc)\n",
    "h,ph=splt.plot_np_percentiles(common_timestamps,[p.values.to_numpy() for p in p_result[1:-1]],base_color=(0.4,0.45,0.5))\n",
    "\n",
    "plt.plot(common_timestamps,ts_partitions[50].average(ta_percentiles).values.to_numpy(), label='year 1980') # note the .average trick here\n",
    "plt.plot(common_timestamps,ts_partitions[60].average(ta_percentiles).values.to_numpy(), label='year 1990')\n",
    "plt.plot(common_timestamps,p_result[0].values.to_numpy(),'b,',label='min-extreme')\n",
    "plt.plot(common_timestamps,p_result[-1].values.to_numpy(),'r,',label='max-extreme')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, this was fun!\n",
    "#, now let's compute the accumulated yearly signals, starting from 1.9\n",
    "ts_acc_partitions = TsVector() # collect the acc stuff here, so we can do percentiles afterwards\n",
    "ta_accumulate = TimeAxis(partition_t0,deltahours(24),365+31) # accumulate from 1.9 + 1 + year and one month\n",
    "for ts in ts_partitions:\n",
    "    ts_acc_partitions.append( ts.accumulate(ta_accumulate))\n",
    "\n",
    "acc_wanted_percentiles = [stat_p.MIN_EXTREME, 10, 25, stat_p.AVERAGE, 50, 75, 90,stat_p.MAX_EXTREME] \n",
    "\n",
    "p_acc_result = percentiles(ts_acc_partitions,ta_accumulate,acc_wanted_percentiles)\n",
    "\n",
    "#then just plot it out, again, we plot from 1.10, even if we aligne to 1.9 re-using ta_percentiles from above\n",
    "common_timestamps = [datetime.datetime.utcfromtimestamp(p.start) for p in ta_accumulate]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(16,8))\n",
    "splt.set_calendar_formatter(utc)\n",
    "h,ph=splt.plot_np_percentiles(common_timestamps,[p.values.to_numpy() for p in p_acc_result[1:-1]],base_color=(0.4,0.45,0.4))\n",
    "\n",
    "plt.plot(common_timestamps,ts_acc_partitions[50].average(ta_accumulate).values.to_numpy(), label='year 1980') # note the .average trick here\n",
    "plt.plot(common_timestamps,ts_acc_partitions[60].average(ta_accumulate).values.to_numpy(), label='year 1990')\n",
    "plt.plot(common_timestamps,p_acc_result[0].values.to_numpy(),'b,',label='min-extreme')\n",
    "plt.plot(common_timestamps,p_acc_result[-1].values.to_numpy(),'r,',label='max-extreme')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
