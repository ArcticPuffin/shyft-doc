{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation with the Shyft api\n",
    "\n",
    "## Introduction\n",
    "At its core, Shyft provides functionality through an API (Application Programming Interface). All the functionality of Shyft is available through this API.\n",
    "\n",
    "We begin the tutorials by introducing the API as it provides the building blocks for the framework. Once you have a good understan\n",
    "\n",
    "In [Part I](run_nea_nidelva.ipynb) of the simulation tutorials, we covered conducting a very simple simulation of an example catchment using configuration files. This is a typical use case, but assumes that you have a model well configured and ready for simulation. In practice, one is interested in working with the model, testing different configurations, and evaluating different data sources.\n",
    "\n",
    "This is in fact a key idea of Shyft -- to make it simple to evaluate the impact of the selection of model routine on the performance of the simulation. In this notebook we walk through a lower level paradigm of working with the toolbox and using the Shyft API directly to conduct the simulations.\n",
    "\n",
    "**This notebook is guiding through the simulation process of a catchment. The following steps are described:**\n",
    "1. **Loading required python modules and setting path to SHyFT installation**\n",
    "2. **Running of a Shyft simulation**\n",
    "3. **Running a Shyft simulation with updated parameters**\n",
    "4. **Activating the simulation only for selected catchments**\n",
    "5. **Setting up different input datasets**\n",
    "6. **Changing state collection settings**\n",
    "7. **Post processing and extracting results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading required python modules and setting path to SHyFT installation\n",
    "\n",
    "Shyft requires a number of different modules to be loaded as part of the package. Below, we describe the required steps for loading the modules, and note that some steps are only required for the use of the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pure python modules and jupyter notebook functionality\n",
    "# first you should import the third-party python modules which you'll use later on\n",
    "# the first line enables that figures are shown inline, directly in the notebook\n",
    "%matplotlib inline\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Shyft Environment\n",
    "\n",
    "This next step is highly specific on how and where you have installed Shyft. If you have followed the guidelines at github, and cloned the three shyft repositories: i) shyft, ii) shyft-data, and iii) shyft-doc, then you may need to tell jupyter notebooks where to find shyft. Uncomment the relevant lines below.\n",
    "\n",
    "If you have a 'system' shyft, or used `conda install -s sigbjorn shyft` to install shyft, then you probably will want to make sure you have set the SHYFTDATA directory correctly, as otherwise, Shyft will assume the above structure and fail. __This has to be done _before_ `import shyft`__. In that case, uncomment the relevant lines below.\n",
    "\n",
    "**note**: it is most likely that you'll need to do one or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\statkraft_data\n"
     ]
    }
   ],
   "source": [
    "# now we create the shyft specific environment\n",
    "# set the path for your shyft build\n",
    "# this should point to the directory that is created\n",
    "# when you clone shyft, assuming you have built shyft\n",
    "# there and not installed it to your system python\n",
    "# if you followed the recommendations in the README, then\n",
    "# you will have cloned three git repos in a parallel structure\n",
    "# and can point to the shyft repository:\n",
    "# Note: you could achieve the same by setting a PYTHONPATH\n",
    "\n",
    "# sys.path.insert(0,os.environ['SHYFT_DEPENDENCIES_DIR'])\n",
    "# shyft_path = os.path.abspath(\"../../../shyft\")\n",
    "# sys.path.insert(0, shyft_path)\n",
    "\n",
    "# If you have set up a system shyft installation, or it has\n",
    "# been set up for you somewhere, then you need to tell these\n",
    "# notebooks where to find the data. This is relevant with respect\n",
    "# to how the .yaml configuration files are set up. Set this to\n",
    "# point to the shyft-data directory on your machine.\n",
    "if not os.environ['SHYFTDATA']:\n",
    "    os.environ['SHYFTDATA'] = os.path.join(os.environ['HOME'],'workspace/shyft_workspace/shyft-data')\n",
    "    \n",
    "print(os.environ['SHYFTDATA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\users\\\\jfb\\\\built_shyft\\\\shyft-4.4.1462-py3.6.egg\\\\shyft']\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, r'D:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg')\n",
    "from shyft import api\n",
    "import shyft\n",
    "\n",
    "print(shyft.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<shyft.repository.netcdf.cf_region_model_repository.CFRegionModelRepository at 0x4c587d4668>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_repo = CFRegionModelRepository(SimDict, ModelDict)\n",
    "rm = reg_repo.get_region_model('demo')\n",
    "reg_repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Shyft simulation\n",
    "\n",
    "The purpose of this notebook is to demonstrate the functionality of the Shyft API. This is a **low level** approach. If you undertstand what is presented herein, you'll be well on your way to working with Shyft.\n",
    "\n",
    "If you prefer to take a **high level** approach, you can start by looking at the [Run Nea Nidelva](simulation-yaml.ipynb) notebook. We recommend taking the time to understand the API, however, as it will be of value later if you want to use your own data and create your own repositories.\n",
    "\n",
    "### Orchestration and Repositories\n",
    "A core philosophy of Shyft is that \"Data should live at the source\". What this means, is that we prefer datasets to either remain in their original format or even come directly from the data provider. To accomplish this, we use \"repositories\". You can read more about repositories at the [Shyft Documentation](https://shyft.readthedocs.io/en/latest/orchestration.html).\n",
    "\n",
    "#### Interfaces\n",
    "Because it is our hope that users will create their own repositories to meet the specifications of their own datasets, we provide 'interfaces'. This is a programming concept that you may not be familiar with. The idea is that it is a basic example, or template, of how the class should work. You can use these and your own class can inherit from them, allowing you to override methods to meet your own specifications. We'll explore this as we move through this tutorial. A nice [explanation of interfaces with python is available here](http://masnun.rocks/2017/04/15/interfaces-in-python-protocols-and-abcs/).\n",
    "\n",
    "### Simulation Configuration\n",
    "What is required to set up a simulation? In the following we'll package some basic information into a dictionary that may be used to configure our simualtion. We'll start by creating a couple of dictionaries. These dictionaries will be used to instantiate an existing repository class that was created for demonstration purposes, `CFRegionModelRepository`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to import the repository to use it in a dictionary:\n",
    "from shyft.repository.netcdf.cf_region_model_repository import CFRegionModelRepository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### region and time specificatin\n",
    "\n",
    "The first dictionary essentially establishes the domain of the simulation and the timing. We also specify a repository that is used to read the data that will provide Shyft a `region_model` (discussed below), based on geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next, create the simulation dictionary\n",
    "SimDict = {'start_datetime': \"2013-09-01T00:00:00\",\n",
    "          'run_time_step': 86400, # seconds, daily\n",
    "          'number_of_steps': 365, # one year\n",
    "          'region_model_id': 'demo', #a unique name identifier of the simulation\n",
    "          'domain': {'EPSG': 32633,\n",
    "                        'nx': 400,\n",
    "                        'ny': 80,\n",
    "                        'step_x': 1000,\n",
    "                        'step_y': 1000,\n",
    "                        'lower_left_x': 100000,\n",
    "                        'lower_left_y': 6960000},\n",
    "          'repository': {'class': shyft.repository.netcdf.cf_region_model_repository.CFRegionModelRepository,\n",
    "                             'params': {'data_file': 'shyft-data/netcdf/orchestration-testdata/cell_data.nc'}},\n",
    "\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, create the simulation dictionary\n",
    "RegionDict = {'region_model_id': 'demo', #a unique name identifier of the simulation\n",
    "              'domain': {'EPSG': 32633,\n",
    "                        'nx': 400,\n",
    "                        'ny': 80,\n",
    "                        'step_x': 1000,\n",
    "                        'step_y': 1000,\n",
    "                        'lower_left_x': 100000,\n",
    "                        'lower_left_y': 6960000},\n",
    "              'repository': {'class': shyft.repository.netcdf.cf_region_model_repository.CFRegionModelRepository,\n",
    "                             'params': {'data_file': 'shyft-data/netcdf/orchestration-testdata/cell_data.nc'}},\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first keys, are probably quite clear:\n",
    "\n",
    "* `start_datetime`: a string in the format: \"2013-09-01T00:00:00\"\n",
    "* `run_time_step`: an integer representing the time step of the simulation (in seconds), so for a daily step: 86400\n",
    "* `number_of_steps`: an integer for how long the simulatoin should run: 365 (for a year long simulation)\n",
    "* `region_model_id`: a string to name the simulation: 'neanidelva-ptgsk'\n",
    "\n",
    "We also need to know *where* the simulation is taking place. This information is contained in the `domain`:\n",
    "\n",
    "* `EPSG`: an EPSG string to identify the coordinate system\n",
    "* `nx`: number of 'cells' in the x direction\n",
    "* `ny`: number of 'cells' in the y direction\n",
    "* `step_x`: size of cell in x direction (m)\n",
    "* `step_y`: size of cell in y direction (m)\n",
    "* `lower_left_x`: where (x) in the EPSG system the cells begin\n",
    "* `lower_left_y`: where (y) in the EPSG system the cells begin\n",
    "* `repository`: a repository that can read the file containing data for the cells (in this case it will read a netcdf file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model specification\n",
    "\n",
    "The next dictionary provides information about the model that we would like to use in Shyft, or the 'Model Stack' as it is generally referred to. In this case, we are going to use the PTGSK model, and the rest of the dictionary provides the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ModelDict = {'model_t': shyft.api.pt_gs_k.PTGSKModel,  # model to construct\n",
    "            'model_parameters': {\n",
    "                'actual_evapotranspiration':{\n",
    "                    'ae_scale_factor': 1.5},\n",
    "                'gamma_snow':{\n",
    "                    'calculate_iso_pot_energy': False,\n",
    "                    'fast_albedo_decay_rate': 6.752787747748934,\n",
    "                    'glacier_albedo': 0.4,\n",
    "                    'initial_bare_ground_fraction': 0.04,\n",
    "                    'max_albedo': 0.9,\n",
    "                    'max_water': 0.1,\n",
    "                    'min_albedo': 0.6,\n",
    "                    'slow_albedo_decay_rate': 37.17325702015658,\n",
    "                    'snow_cv': 0.4,\n",
    "                    'tx': -0.5752881492890207,\n",
    "                    'snowfall_reset_depth': 5.0,\n",
    "                    'surface_magnitude': 30.0,\n",
    "                    'wind_const': 1.0,\n",
    "                    'wind_scale': 1.8959672005350063,\n",
    "                    'winter_end_day_of_year': 100},\n",
    "                'kirchner':{ \n",
    "                    'c1': -3.336197322290274,\n",
    "                    'c2': 0.33433661533385695,\n",
    "                    'c3': -0.12503959620315988},\n",
    "                'precipitation_correction': {\n",
    "                    'scale_factor': 1.0},\n",
    "                'priestley_taylor':{'albedo': 0.2,\n",
    "                    'alpha': 1.26},\n",
    "                    }\n",
    "            }               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dictionary we define two variables:\n",
    "\n",
    "* `model_t`: the import path to a shyft 'model stack' class\n",
    "* `model_parameters`: a dictionary containing specific parameter values for a particular model class\n",
    "\n",
    "Specifics of the `model_parameters` dictionary will vary based on which class is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so far we have two dictionaries. One which provides information regarding our simulation domain, and a second which provides information on the model that we wish to run over the domain (e.g. in each of the cells). The next step, then, is to map these together and create a `region_repo` class.\n",
    "\n",
    "This is achieved by using a repository, in this case, the `CFRegionModelRepository` we imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_repo = CFRegionModelRepository(RegionDict, ModelDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `region_model`\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**TODO:** a notebook documenting the CFRegionModelRepository\n",
    "\n",
    "</div>\n",
    "\n",
    "The first step in conducting a hydrologic simulation is to define the **domain of the simulation** and the **model type** which we would like to simulate. To do this we create a `region_model` object. Above we created dictionaries that can provide this information. In this next step, we put it together so that we have a single object which we can work with \"at our fingertips\". You'll note above that we have pointed to a 'data_file' earlier when we defined the `SimDict`. This data file contains all the required elements to fill the cells of our domain. The informaiton is contained in a single [netcdf file](../../../shyft-data/netcdf/orchestration-testdata/cell_data.nc)\n",
    "\n",
    "Before we go further, let's look briefly at the contents of this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    dimensions(sizes): cell(4650)\n",
      "    variables(dimensions): float64 \u001b[4mx\u001b[0m(cell), float64 \u001b[4my\u001b[0m(cell), float64 \u001b[4mz\u001b[0m(cell), int32 \u001b[4mcrs\u001b[0m(), float64 \u001b[4marea\u001b[0m(cell), float64 \u001b[4mforest-fraction\u001b[0m(cell), float64 \u001b[4mreservoir-fraction\u001b[0m(cell), float64 \u001b[4mlake-fraction\u001b[0m(cell), float64 \u001b[4mglacier-fraction\u001b[0m(cell), int32 \u001b[4mcatchment_id\u001b[0m(cell)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cell_data_file = os.path.join(os.environ['SHYFTDATA'], 'shyft-data/netcdf/orchestration-testdata/cell_data.nc')\n",
    "cell_data = Dataset(cell_data_file)\n",
    "print(cell_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be surprised to see the dimensions are 'cells', but recall that in Shyft everything is vectorized. Each 'cell' is an element within a domain, and each cell has associated variables:\n",
    "\n",
    "* location: x, y, z\n",
    "* characteristics: forest-fraction, reservoir-fraction, lake-fraction, glacier-fraction, catchment-id\n",
    "\n",
    "We'll bring this data into our workspace via the `region_model`. Note that we have instantiated a `region_repo` class using one of the existing Shyft repositories, in this case one that was built for reading in the data as it is contained in the example [shyft-data](https://github.com/statkraft/shyft-data) netcdf files: `CFRegionModelRepository`.\n",
    "\n",
    "Next, we'll use the `region_repo.get_region_model` method to get the `region_model`. Note the name 'demo', in this case is arbitrary. However, depending on how you create your repository, you can specify what region model to return using this string.\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "\n",
    "**note:** *you are strongly encouraged to learn how to create repositories. This particular repository is just for demonstration purposes. In practice, one may use a repository that connects directly to a GIS service, a database, or some other data sets that contain the data required for simulations.*\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**warning**: *also, please note that below we call the 'get_region_model' method as we instantiate the class. This behavior may change in the future.*\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_model = region_repo.get_region_model('demo')\n",
    "region_model.time_axis.start\n",
    "\n",
    "period = region_model.time_axis.total_period()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-oo,-oo>\n"
     ]
    }
   ],
   "source": [
    "print(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the `region_model`\n",
    "\n",
    "So we now have created a `region_model`, but what is it actually? This is a very **fundamental class** in Shyft. Essentially, the `region_model` contains all the information regarding the simulation type and domain. There are many methods associated with the `region_model` and it will take time to understand all of them. For now, let's just explore a few key methods:\n",
    "\n",
    "* `bounding_region`: provides information regarding the domain of interest for the simulation\n",
    "* `catchment_id_map`: indices of the various catchments within the domain\n",
    "* `cells`: an instance of `PTGSKCellAllVector` that holds the individual cells for the simulation (*note that this is type-specific to the model type*)\n",
    "* `ncore`: an integer that sets the numbers of cores to use during simulation (Shyft is very greedy if you let it!)\n",
    "* `time_axis`: a `shyft.api.TimeAxisFixedDeltaT` class (basically contains information regarding the timing of the simulation)\n",
    "\n",
    "Keep in mind that many of these methods are more 'C++'-like than 'Pythonic'. This means, that in some cases, you'll have to 'call' the method. For example: `region_model.bounding_region.epsg()` returns a string. You can use tab-completion to explore the `region_model` further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32633'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_model.bounding_region.epsg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll likely note that there are a number of intriguing fucntions, e.g. `initialize_cell_environment` or `interpolate`. But before we can go further, we need some more information. Perhaps you are wondering about forcing data. So far, we haven't said anything about **model input**, we've only set up a container that holds all the information about our simulation. Still, we have made *some* progress. Let's look for instance at the cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoCellData(mid_point=GeoPoint(204843.7371537306,6994695.209048475,978.344970703125),catchment_id=2305,area=209211.92418108872,ltf=LandTypeFractions(glacier=0.0,lake=0.0,reservoir=0.0,forest=0.0,unspecified=1.0))\n"
     ]
    }
   ],
   "source": [
    "cell_0 = region_model.cells[0]\n",
    "print(cell_0.geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that so far, each of the cells in the region_model contain information regarding their LandTypeFractions, geolocation, catchment_id, and area. \n",
    "\n",
    "There is a particulary important attribute of the cells: `env_ts`. This is a container for each cell that holds the \"environmental timeseries\", or forcing data, for the simulation. The container is there, and it is customized to provide timeseries as required by the model type we selected, in this case: `.PTGSKModel` (see the `ModelDict`. So for every cell in your simulation, there is a container prepared to accept the forcing data as the next cell shows. So now we'll continue on and populate these containers with forcing data for our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['init', 'precipitation', 'radiation', 'rel_hum', 'temperature', 'wind_speed']\n"
     ]
    }
   ],
   "source": [
    "print([d for d  in dir(cell_0.env_ts) if  '_'  not in d[0]]) #just so we don't see 'private' attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding forcing data to the `region_model`\n",
    "\n",
    "Clearly the next step is to add forcing data to our `region_model` object. This is where some of the 'magic' of Shyft starts to shine. We haven't yet focused on any particularly special functionality of Shyft. But hopefully you'll begin to see some of the power behind these configuration classes as we add data. Let's start by thinking about what kind of data we need. From above, it's clear that this particular model stack, `PTGSKModel`, requires:\n",
    "\n",
    "* precipitation\n",
    "* radiation\n",
    "* relative humidity (rel_hum)\n",
    "* temperature\n",
    "* wind speed\n",
    "\n",
    "We have stored this information each in seperate netcdf files. Again, these files **do not represent the recommended practice**, but are *only for demonstration purposes*. The idea here is just to demonstrate with an example repository, but *you should create your own to match **your** data*.\n",
    "\n",
    "#### A word on units\n",
    "Units in Shyft follow ........... [TODO]\n",
    "\n",
    "#### \"Sources\"\n",
    "\n",
    "We use the term *sources* to define a location data may be coming from. You may also come across *destinations*. In both cases, it just means a file, database, service of some kind, etc. that is capable of providing data. Repositories are written to connect to *sources*. Following our earlier approach, we'll create another dictionary to define our data sources, but first we need to import another repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shyft.repository.netcdf.cf_geo_ts_repository import CFDataRepository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shyft.repository.netcdf.cf_geo_ts_repository import CFDataRepository\n",
    "Datasets = {'sources': [\n",
    "        \n",
    "    {'repository': shyft.repository.netcdf.cf_geo_ts_repository.CFDataRepository,\n",
    "     'params': {'epsg': 32633,\n",
    "            'selection_criteria': None,\n",
    "            'stations_met': 'shyft-data/netcdf/orchestration-testdata/precipitation.nc'},\n",
    "     'types': ['precipitation']},\n",
    "       \n",
    "    {'repository': shyft.repository.netcdf.cf_geo_ts_repository.CFDataRepository,\n",
    "     'params': {'epsg': 32633,\n",
    "            'selection_criteria': None,\n",
    "            'stations_met': 'shyft-data/netcdf/orchestration-testdata/temperature.nc'},\n",
    "    'types': ['temperature']},\n",
    "        \n",
    "    {'params': {'epsg': 32633,\n",
    "            'selection_criteria': None,\n",
    "            'stations_met': 'shyft-data/netcdf/orchestration-testdata/wind_speed.nc'},\n",
    "     'repository': shyft.repository.netcdf.cf_geo_ts_repository.CFDataRepository,\n",
    "     'types': ['wind_speed']},\n",
    "    \n",
    "    {'repository': shyft.repository.netcdf.cf_geo_ts_repository.CFDataRepository,\n",
    "     'params': {'epsg': 32633,\n",
    "            'selection_criteria': None,\n",
    "            'stations_met': 'shyft-data/netcdf/orchestration-testdata/relative_humidity.nc'},\n",
    "     'types': ['relative_humidity']},\n",
    "    \n",
    "    {'repository': shyft.repository.netcdf.cf_geo_ts_repository.CFDataRepository,\n",
    "     'params': {'epsg': 32633,\n",
    "            'selection_criteria': None,\n",
    "            'stations_met': 'shyft-data/netcdf/orchestration-testdata/radiation.nc'},\n",
    "     'types': ['radiation']}]\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Repositories\n",
    "\n",
    "In another notebook, further information will be provided regarding the repositories. For the time being, let's look at this configuration dictionary that was created. It essentially just contains a list, keyed by the name `\"sources\"`. This key is known in some of the tools that are built in the Shyft orchestration, so it is recommended to use it.\n",
    "\n",
    "Each item in the list is a dictionary for each of the source types, the keys in the dictionaries are: `repository`, `params`, and `types`. The general idea and concept is that in orchestration, the object keyed by `repository` is a class that is instantiated by passing the objects contained in `params`.\n",
    "\n",
    "Let's repeat that. From our `Datasets` dictionary, we get a list of `\"sources\"`. Each of these sources contains a class (a repository) that is capable of getting the source data into Shyft. Whatever parameters that are required for the class to work, will be included in the `\"sources\"` dictionary. In our case, the `params` are quite simple, just a path to a netcdf file. But suppose our repository required credentials or other information for a database? This information could also be included in the `params` stanza of the dictionary.\n",
    "\n",
    "You should explore the above referenced netcdf files that are available at the [shyft-data](https://github.com/statkraft/shyft-data) git repository. These files contain the forcing data that will be used in the example simulation. Each one contains observational data from some stations in our catchment. Depending on how you write your repository, this data may be provided to Shyft in many different formats.\n",
    "\n",
    "Let's explore this concept further by getting the 'temperature' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the temperature sources:\n",
    "tmp_sources = [source for source in Datasets['sources'] if 'temperature' in source['types']]\n",
    "\n",
    "# in this example there is only one\n",
    "t0 = tmp_sources[0]\n",
    "\n",
    "# We will now instantiate the repository with the parameters that are provided\n",
    "# in the dictionary. \n",
    "# Note the 'call' structure expects params to contain keyword arguments, and these\n",
    "# can be anything you want depending on how you create your repository\n",
    "tmp_repo = t0['repository'](**t0['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tmp_repo` is now an instance of the Shyft `CFDataRepository`, and this will provide Shyft with the data when it sets up a simulation.\n",
    "\n",
    "Now that we have set up our `region_model` and we have a set of repositories available for reading our data, we can start to set up a simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our `tmp_repo` has a method called `get_timeseries`. This method requires a list of the time series types to return :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 100000.,  500000.,  500000.,  100000.]), array([ 6960000.,  6960000.,  7040000.,  7040000.]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-cdd4abb00543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \"relative_humidity\", \"radiation\")\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# help(tmp_repo.get_timeseries)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_repo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_geo_ts_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_location_criteria\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg\\shyft\\repository\\netcdf\\cf_geo_ts_repository.py\u001b[0m in \u001b[0;36mget_timeseries\u001b[1;34m(self, input_source_types, utc_period, geo_location_criteria)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             return self._get_data_from_dataset(dataset, input_source_types,\n\u001b[1;32m--> 105\u001b[1;33m                                                utc_period, geo_location_criteria)\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_source_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutc_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_location_criteria\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg\\shyft\\repository\\netcdf\\cf_geo_ts_repository.py\u001b[0m in \u001b[0;36m_get_data_from_dataset\u001b[1;34m(self, dataset, input_source_types, utc_period, geo_location_criteria, ensemble_member)\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCFDataRepositoryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not find all data fields\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mextracted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0missubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0missubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextracted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextracted_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_geo_ts_to_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_timeseries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg\\shyft\\repository\\netcdf\\cf_geo_ts_repository.py\u001b[0m in \u001b[0;36m_transform_raw\u001b[1;34m(self, data, time, issubset)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mak\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mak\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg\\shyft\\repository\\netcdf\\cf_geo_ts_repository.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m    335\u001b[0m         convert_map = {\"wind_speed\": lambda x, t: (noop_space(x), noop_time(t)),\n\u001b[0;32m    336\u001b[0m                        \u001b[1;34m\"relative_humidity\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnoop_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoop_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m                        \u001b[1;34m\"temperature\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnoop_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoop_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m                        \u001b[1;34m\"global_radiation\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnoop_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoop_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m                        \u001b[1;34m\"precipitation\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnoop_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoop_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\users\\jfb\\built_shyft\\shyft-4.4.1462-py3.6.egg\\shyft\\repository\\netcdf\\cf_geo_ts_repository.py\u001b[0m in \u001b[0;36mnoop_time\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mnoop_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeAxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUtcTimeVector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdacc_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "bbox = region_model.bounding_region.bounding_box(region_model.bounding_region.epsg())\n",
    "print(bbox)\n",
    "period = region_model.time_axis.total_period()\n",
    "_geo_ts_names = (\"temperature\", \"wind_speed\", \"precipitation\",\n",
    "                              \"relative_humidity\", \"radiation\")\n",
    "# help(tmp_repo.get_timeseries)\n",
    "source = tmp_repo.get_timeseries(_geo_ts_names, period, geo_location_criteria=bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll actually create a collection of repositories, as we have different input types.\n",
    "from shyft.repository.geo_ts_repository_collection import GeoTsRepositoryCollection\n",
    "\n",
    "def construct_geots_repo(datasets_config, epsg=None):\n",
    "    \"\"\" iterates over the different sources that are provided \n",
    "    and prepares the repository to read the data for each type\"\"\"\n",
    "    geo_ts_repos = []\n",
    "    src_types_to_extract = []\n",
    "    for source in datasets_config['sources']:\n",
    "        if epsg is not None:\n",
    "            source['params'].update({'epsg': epsg})\n",
    "        geo_ts_repos.append(source['repository'](**source['params']))\n",
    "        src_types_to_extract.append(source['types'])\n",
    "    \n",
    "    return GeoTsRepositoryCollection(geo_ts_repos, src_types_per_repo=src_types_to_extract)\n",
    "\n",
    "# instantiate the repository\n",
    "geots_repo = construct_geots_repo(Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         if time_axis is None:\n",
    "#             time_axis = self.time_axis\n",
    "#         else:\n",
    "#             self.region_model.initialize_cell_environment(time_axis)\n",
    "#         self.region_model.initial_state = self.get_initial_state_from_repo() if state is None else state\n",
    "#         bbox = self.region_model.bounding_region.bounding_box(self.epsg)\n",
    "#         period = time_axis.total_period()\n",
    "#         sources = self.geo_ts_repository.get_timeseries(self._geo_ts_names, period,\n",
    "#                                                         geo_location_criteria=bbox)\n",
    "#         self.region_model.region_env = self._get_region_environment(sources)\n",
    "#         self.region_model.interpolation_parameter = self.ip_repos.get_parameters(self.interpolation_id)\n",
    "#         self.simulate()\n",
    "# def _get_region_environment(self, sources):\n",
    "#     region_env = api.ARegionEnvironment()\n",
    "#     region_env.temperature = sources[\"temperature\"]\n",
    "#     region_env.precipitation = sources[\"precipitation\"]\n",
    "#     region_env.radiation = sources[\"radiation\"]\n",
    "#     region_env.wind_speed = sources[\"wind_speed\"]\n",
    "#     region_env.rel_hum = sources[\"relative_humidity\"]\n",
    "#     return region_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how to set up a Shyft simulation using the `yaml_configs.YAMLSimConfig` class. Note that this is a **high level** approach, providing a working example for a simple simulation. More advanced users will want to eventually make use of direct API calls, as outlined in [Part II](advanced_simulation.ipynb).\n",
    "\n",
    "At this point, you may want to have a look to the [configuration file](./nea-config/neanidelva_simulation.yaml) used in this example.\n",
    "\n",
    "```\n",
    "---\n",
    "neanidelva:\n",
    "  region_config_file: neanidelva_region.yaml\n",
    "  model_config_file: neanidelva_model_calibrated.yaml\n",
    "  datasets_config_file: neanidelva_datasets.yaml\n",
    "  interpolation_config_file: neanidelva_interpolation.yaml\n",
    "  start_datetime: 2013-09-01T00:00:00\n",
    "  run_time_step: 86400  # 1 hour time step\n",
    "  number_of_steps: 365  # 1 year\n",
    "  region_model_id: 'neanidelva-ptgsk'\n",
    "  #interpolation_id: 2   # this is optional (default 0)\n",
    "  initial_state:\n",
    "    repository:\n",
    "      class: !!python/name:shyft.repository.generated_state_repository.GeneratedStateRepository\n",
    "      params:\n",
    "        model: !!python/name:shyft.api.pt_gs_k.PTGSKModel\n",
    "    tags: []\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "The file is structured as follows:\n",
    "\n",
    "`neanidelva` is the name of the simulation. Your configuration file may contain multiple \"stanzas\" or blocks of simulation configurations. You'll see below that we use the name to instantiate a configuration object.\n",
    "\n",
    "`region_config_file` points to another yaml file that contains basic information about the region of the simulation. You can [explore that file here](./nea-config/neanidelva_region.yaml)\n",
    "\n",
    "`model_config_file` contains the model parameters. Note that when you are calibrating the model, [this is the file](./nea-config/neanidelva_model_calibrated.yaml) that you would put your optimized parameters into once you have completed a calibrations.\n",
    "\n",
    "`datasets_config_file` contains details regarding the input datasets and the [repositories](../../repositories.rst) they are contained in. You can see [this file here](./nea-config/neanidelva_datasets.yaml)\n",
    "\n",
    "`interpolation_config_file` provides details regarding how the observational data in your catchment or region will be interpolated to the domain of the simulation. If you are using a repository with distributed data, the interpolation is still used. [See this file](./nea-config/neanidelva_interpolation.yaml) for more details.\n",
    "\n",
    "The following:\n",
    "\n",
    "```\n",
    "  start_datetime: 2013-09-01T00:00:00\n",
    "  run_time_step: 86400  # 1 hour time step\n",
    "  number_of_steps: 365  # 1 year\n",
    "  region_model_id: 'neanidelva-ptgsk'\n",
    "```\n",
    "\n",
    "are considered self-explantory. Note that `region_model_id` is simply a string name, but it should be **unique**. We will explain the details regarding `initial_state` later on in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shyft imports\n",
    "from shyft import api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Shyft API\n",
    "In the following section we'll explore several components of the `shyft.api`. We consider the strength of Shyft to lie within this Application Programming Interface, or API. To the uninitiated, it adds quite a degree of complexity. However, once you understand the different components and paradigms of Shyft, you'll see the flexibility the API offers provides a great number of possibilities for exploring hydrologic simulations.\n",
    "\n",
    "To start, as in the first tutorial, we'll start with a `YAMLSimConfig` object and create a `ConfigSimulator`. This will get us going quickly and provide a `region_model` as a starting point. But in this tutorial, the point is to work more closely with different components of the `Shyft.api`. In particular, we hope to demonstrate how the underlying `region_model`, rather than a `simulator`, is developed. The latter is just a wrapper that is made for convenience when running models operationally. If one is interested in working with Shyft to explore algorithm performance, it is easier to work directly with the `region_model` class.\n",
    "\n",
    "The API approach will take a bit more code to get started, but will allow great flexibility later on. The first thing we'll do is to expose several of the attributes (which are mostly Shyft API classes) of the `simulator` object. Let's begin by getting those in our namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expose attributes of the simulator\n",
    "region_model = simulator.region_model\n",
    "region_model_id = simulator.region_model_id\n",
    "interpolation_id = simulator.interpolation_id\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [first tutorial](run_nea_nidelva.ipynb#The-simulator-and-the-region_model) we discussed the `region_model`. If you are unfamiliar with this class, we recommend reviewing the [description](region_model.rst). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The concept of Shyft repositories\n",
    "\n",
    "In Shyft, we consider that input data is a \"source\". Our source data resides in some kind of data serialization... be it a text file, netcdf file, or database... One could have any kind of storage format for the source data. [Repositories](repositories.rst) are Python based interfaces to data. Several have been created within Shyft already, but users are encouraged to create their own. A guiding paradigm to Shyft is that data should live as close to the source as possible (ideally, at the source). The repositories connect to the data source and make the data available to Shyft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expose the repositories\n",
    "region_model_repo = simulator.region_model_repository\n",
    "interpolation_param_repo = simulator.ip_repos\n",
    "geo_ts_repo = simulator.geo_ts_repository\n",
    "initial_state_repo = simulator.initial_state_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have exposed the **repositories** that we connected to our `region_model` during configuration. Having access to the repositories, means that we have access to the input data sources directly (found in `geo_ts_repository`). We also have several other repositories, including a repository for the interpolation parameters, initial state, and the region_model. We'll explore some of these a bit deeper now. But first we'll expose a few more pieces of information from the region_model while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsg = region_model.bounding_region.epsg()\n",
    "bbox = region_model.bounding_region.bounding_box(epsg)\n",
    "period = region_model.time_axis.total_period()\n",
    "geo_ts_names = (\"temperature\", \"wind_speed\", \"precipitation\", \"relative_humidity\", \"radiation\")\n",
    "\n",
    "sources = shyft.repository.netcdf.cf_geo_ts_repository.geo_ts_repo.get_timeseries(geo_ts_names, period, geo_location_criteria=bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `epsg` is simply the domain projection information for our simulation. `bbox` provides the bounding box coordinates. `period` gives the total period of the simulation. Lastly, we create a tuple of the 'geolocated timeseries names' or `geo_ts_names` as it is referred to here. And use this to get the *sources* out of our repository. Note that these names:\n",
    "\n",
    "    temperature\n",
    "    wind_speed\n",
    "    precipitation\n",
    "    relative_humidity\n",
    "    radiation\n",
    "    \n",
    "Are embedded into Shyft as timeseries names that are required for simulations. In the current implementations, these are the default names used in repositories and, at present, the only forcing data required. If one were to develop new algorithms that reqiured other forcings, you would need to define these in a custom repository. See `interfaces.py` for more details.\n",
    "\n",
    "Before going further, let's look at what we have so far...\n",
    "\n",
    "We won't look in detail at all the repositories, but let's take a look at the `geo_ts_repo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore geo_ts_repo\n",
    "#help(geo_ts_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `geo_ts_repo` is a collection of geolocated timeseries repositories. Note that `geo_ts_repo` has an attribute: `.geo_ts_repositories`... this seems redundant? This is simply a list of the repositories this class is 'managing'.\n",
    "\n",
    "Maybe we want to look at the precipitation input series in more detail. We can get at those via this class. NOTE, this may not be the most typical way to look at your input data (presumably you may have already done this before the simulation working with the raw netcdf files), but in case you wish to see the datasets from the \"model\" perspective, this is how you gain access. Also, maybe you want to conduct a simulation, then make a data correction. You could do that by accessing the values here. Each of the aforementioned series types have a specialized source vector type in Shyft. In the case of precipitation it is a `api.PrecipitationSourceVector`. If we dig into this, we'll find some aspects familiar from the first tutorial.\n",
    "\n",
    "Let's get the precipitation timeseries out of the repository for the period of the simulation first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# above we already created a `sources` dictionary by\n",
    "# using the `get_timeseries` method. This method takes a \n",
    "# list of the timeseries names as input and a period \n",
    "# of type 'shyft.api._api.UtcPeriod'\n",
    "# it returns a dictionary, keyed by the names of the timeseries\n",
    "prec = sources['precipitation']\n",
    "\n",
    "# `prec` is now a `api.PrecipitationSourceVector` and if you look\n",
    "# you'll see it 10 elements:\n",
    "print(len(prec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore further and see each element is in itself an `api.PrecipitationSource`, which has a timeseries (ts). Recall from the [first tutorial](run_nea_nidelva.ipynb#Visualizing-the-discharge-for-each-[sub-]catchment) that we can easily convert the `timeseries.time_axis` into datetime values for plotting.\n",
    "\n",
    "Let's plot the precip of each of the sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "for pr in prec:\n",
    "    t,p = [dt.datetime.utcfromtimestamp(t_.start) for t_ in pr.ts.time_axis], pr.ts.values\n",
    "    ax.plot(t,p, label=pr.mid_point().x) #uid is empty now, but we reserve for later use\n",
    "fig.autofmt_xdate()\n",
    "ax.legend(title=\"Precipitation Input Sources\")\n",
    "ax.set_ylabel(\"precip[mm/hr]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we leave this section, we can also take a quick look at the `interpolation_param_repo`. This is a different type of repository, and it contains the parameters that will be passed to the interpolation algorithm to take a point-source timeseries and interpolate them to the Shyft cells, or in the context of of the API: `region_model.cells`. We'll quickly look at the `.params` attribute, which is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolation_param_repo.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quickly recognizes the same input source type keywords that are used as keys to the `params` dictionary. `params` is simply a dictionary of dictionaries which contains the parameters used by the interpolation model that is specific for each source type.\n",
    "\n",
    "In closing, one is encouraged to understand well the concept of the **repositories**. As a user of Shyft, it is likely you'll want to create your own repository to access your data directly rather than creating input files for Shyft. Keep in mind that the repositories are Python code, and not a part of the core C++ code of Shyft. They are designed to provide an interface between the C++ code and potentially more 'pythonic' paradigms. In the following section, you'll see that we populate a C++ class from a repository collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ARegionEnvironment class\n",
    "\n",
    "The next thing we'll do is to create an `api.ARegionEnvironment` class to use in our custom simulation. As the `geo_ts_repo` was a Python interface that provided a collection of all the timeseries repositories, the `region_env` is an API type that provides a container of the \"sources\" of data specific to the model. We will now create an `api.ARegionEnvironment` from the `geo_ts_repo`. It may be helpful to think of a `region_env` as the container of input data for the  `region_model` -- in fact, that is what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_region_env(sources_):\n",
    "    region_env_ = api.ARegionEnvironment()\n",
    "    region_env_.temperature = sources_[\"temperature\"]\n",
    "    region_env_.precipitation = sources_[\"precipitation\"]\n",
    "    region_env_.radiation = sources_[\"radiation\"]\n",
    "    region_env_.wind_speed = sources_[\"wind_speed\"]\n",
    "    region_env_.rel_hum = sources_[\"relative_humidity\"]\n",
    "    return region_env_\n",
    "\n",
    "region_env = get_region_env(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done here is to convert our input data from the Python based repositories into a C++ type object that is used in the Shyft core. It may feel redundant to `geo_ts_repo`, but there are underlying differences. Still, you'll see that now the 'sources' are direct attributes of the `region_env` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(region_env.precipitation))\n",
    "type(region_env.precipitation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation Parameters\n",
    "In the same manner that we need to convert the sources from the Python based container, we'll also create an API object from the `interpolation_param_repo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpolation_parameters = interpolation_param_repo.get_parameters(interpolation_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are set to *rebuild* our `region_model` from scratch. In the next few steps we're going to walk through initialization of the `region_model` to set it up for simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the region_model\n",
    "The two `shyft.api` types: `api.ARegionEnvironment` and `api.InterpolationParameter` together are used to initialize the `region_model`. In the next step, all of the timeseries input sources are interpolated to the geolocated model cells. After this step, each cell is the model has it's own `env_ts` which contains the timeseries for that cell. Let's first do the interpolation, the we can explore the `region_model.cells` a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#region_model.run_interpolation(interpolation_parameters, region_model.time_axis, region_env)\n",
    "region_model.interpolate(interpolation_parameters, region_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was simple. Let's look at the timeseries in some individual cells. The following is a bit of a contrived example, but it shows some aspects of the api. We'll plot the temperature series of all the cells in one sub-catchment, and color them by elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.cm import jet as jet\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# get all the cells for one sub-catchment with 'id' == 1228\n",
    "c1228 = [c for c in region_model.cells if c.geo.catchment_id() == 1228]\n",
    "\n",
    "# for plotting, create an mpl normalizer based on min,max elevation\n",
    "elv = [c.geo.mid_point().z for c in c1228]\n",
    "norm = Normalize(min(elv), max(elv))\n",
    "\n",
    "#plot with line color a function of elevation\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "# here we are cycling through each of the cells in c1228\n",
    "for dat,elv in zip([c.env_ts.temperature.values for c in c1228], [c.mid_point().z for c in c1228]):\n",
    "    ax.plot(dat, color=jet(norm(elv)), label=int(elv))\n",
    "    \n",
    "    \n",
    "# the following is just to plot the legend entries and not related to Shyft\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# sort by labels\n",
    "import operator\n",
    "hl = sorted(zip(handles, labels),\n",
    "            key=operator.itemgetter(1))\n",
    "handles2, labels2 = zip(*hl)\n",
    "\n",
    "# show legend, but only every fifth entry\n",
    "ax.legend(handles2[::5], labels2[::5], title='Elevation [m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect from the temperature kriging method, we should find higher elevations have colder temperatures. As an exercise you could explore this relationship using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create a function that will read initial states from the `initial_state_repo`. In practice, this is already done by the `ConfgiSimulator`, but to demonstrate lower level functions, we'll reset the states of our `region_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to read the states from the state repository\n",
    "def get_init_state_from_repo(initial_state_repo_, region_model_id_=None, timestamp=None):\n",
    "    state_id = 0\n",
    "    if hasattr(initial_state_repo_, 'n'):  # No stored state, generated on-the-fly\n",
    "        initial_state_repo_.n = region_model.size()\n",
    "    else:\n",
    "        states = initial_state_repo_.find_state(\n",
    "            region_model_id_criteria=region_model_id_,\n",
    "            utc_timestamp_criteria=timestamp)\n",
    "        if len(states) > 0:\n",
    "            state_id = states[0].state_id  # most_recent_state i.e. <= start time\n",
    "        else:\n",
    "            raise Exception('No initial state matching criteria.')\n",
    "    return initial_state_repo_.get_state(state_id)\n",
    " \n",
    "init_state = get_init_state_from_repo(initial_state_repo, region_model_id_=region_model_id, timestamp=region_model.time_axis.start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about the function for now, but do take note of the `init_state` object that we created. This is another container, this time it is a class that contains `PTGSKState` objects, which are specific to the model stack implemented in the simulation (in this case `PTGSK`). If we explore an individual state object, we'll see `init_state` contains, for each cell in our simulation, the state variables for each 'method' of the method stack.\n",
    "\n",
    "Let's look more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_pub_attr(obj):\n",
    "    #only public attributes\n",
    "    print([attr for attr in dir(obj) if attr[0] is not '_']) \n",
    "    \n",
    "print(len(init_state))\n",
    "init_state_cell0 = init_state[0]\n",
    "# gam snow states\n",
    "print_pub_attr(init_state_cell0.gs)\n",
    "\n",
    "#init_state_cell0.kirchner states\n",
    "print_pub_attr(init_state_cell0.kirchner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "We have now explored the `region_model` and looked at how to instantiate a `region_model` by using a `api.ARegionEnvironment`, containing a collection of timeseries sources, and passing an `api.InterpolationParameter` class containing the parameters to use for the data interpolation algorithms. The interpolation step \"populated\" our cells with data from the point sources.\n",
    "\n",
    "The cells each contain all the information related to the simulation (their own timeseries, `env_ts`; their own model parameters, `parameter`; and other attributes and methods). In future tutorials we'll work with the cells indivdual \"resource collector\" (`.rc`) and \"state collector\" (`.sc`) attributes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
